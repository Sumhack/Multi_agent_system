import os


from langchain.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.llms import GooglePalm
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import google.generativeai as genai



genai.configure(api_key="gemini-api-key")
model = genai.GenerativeModel("gemini-1.5-flash")

# Function to load the vector store
def load_vectorstore():
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key="gemini-api-key")
    vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    return vectorstore

# Function to perform similarity search
def perform_similarity_search(vectorstore, query):
    results = vectorstore.similarity_search(query, k=1)
    if results:
        # Extract the result content, source, and page number
        answer = results[0].page_content
        source = results[0].metadata.get('source', 'Unknown source')
        page_number = results[0].metadata.get('page_number', 'Unknown page')
        return answer, source, page_number
    else:
        return None, None, None

# Load vectorstore and LLM
vectorstore = load_vectorstore()

# Define the template
template = """
You are a technical support agent, with skills for Telecom sector, and you are tasked to response to user queries and explain the error and resolution for the same
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use the chat history to maintain context of the conversation.

Chat History: {chat_history}
Context: {context}
Human: {question}
AI Clone:
"""

# Define conversation history and template prompt
history = []

# Create a function to generate the LLM prompt
def create_prompt(chat_history, context, question):
    return template.format(chat_history=chat_history, context=context, question=question)

# Define the chatbot function
def chatbot(input_text):
    global history  # Retain conversation history

    # Check for relevant information in the vector store
    answer, source, page_number = perform_similarity_search(vectorstore, input_text)

    # Build context with the similarity search result (if found)
    if answer:
        # If vector store found relevant content, pass it as context
        context = f"{answer} (Source: {source}, Page: {page_number})"
    else:
        context = "No relevant context found in the database."

    # Format chat history
    chat_history = "\n".join([f"Human: {item[0]}\nAI Clone: {item[1]}" for item in history])

    # Create the LLM prompt using the template
    prompt = create_prompt(chat_history, answer, input_text)
    #print(prompt)

    # Generate the response using LLM with the prompt
    response = model.generate_content(prompt)
    #print(response.text)
    rep = response.text

    # Append the conversation to the history for context
    history.append((input_text, response))

    # Format source information with page number
    if answer:
        source_info = f"Source: {source} (Page {page_number})"
    else:
        source_info = "Source: Generated by AI (No specific document found)"

    return rep, source_info
